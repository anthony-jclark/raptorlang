#!/usr/bin/env python

"""
Create an enum for tokens, and a function for printing them.

TODO:
-

"""

import sys
import os

TAB = "    "
TOK_ENUM = "TOK"
TOK_STRUCT = "Token"

TOSTRING = "to_string"
TOSTRING_FCN = "TOK_" + TOSTRING
RESERVED_FCN = "get_reserved_TOK"
PUNCT_FCN = "get_punctuator_TOK"

GETRESERVED_FCN = "get_list_reserved"
GETPUNCTS_FCN = "get_list_punctuator"

MAX_COL = 80
COMMENT_BANNER = "// " + "-" * 56


def read_tokens(lines):
    """Read tokens from standard input."""

    tokens = []
    keywords = []
    punctuators = []
    other = []
    lnumber = 0

    for line in lines:
        lnumber += 1

        # Strip extraneous white space and check for EOF
        lstrip = line.strip()
        if not lstrip:
            break

        # Remove whitespace from the middle of string and check for comments
        lsplit = lstrip.split()
        if (lsplit[0] == "//"):
            continue

        # Ensure that the ':' and ';' characters are present
        try:
            if (lsplit[1] != ':' or lsplit[3] != ';'):
                msg = "Error parsing the token file on line " + str(lnumber)
                print(msg + ' : "' + " ".join(lsplit) + '".')
                exit(1)
        except Exception, e:
            msg = "Invalid token string on line " + str(lnumber)
            print(msg + ' ("' + " ".join(lsplit) + '""): ' + str(e) + '.')
            exit(1)

        # Add tokens to the dictionary
        tname = lsplit[0]
        tvalue = lsplit[2].replace("'", "")
        token = (tname, tvalue)
        tokens.append(token)

        # Add keywords to list
        if (len(tvalue) != 0 and (tvalue[0].isalpha() or tvalue[0] == '_')):
            if token not in keywords:
                keywords.append(token)
            else:
                print("Token already in keywords: " + str(token))

        elif (len(tvalue) != 0):
            if token not in punctuators:
                punctuators.append(token)
            else:
                print("Punctuator already in punctuators: " + str(token))

        else:
            other.append(token)
            print("Token not a punc or keyword: " + str(token))

    # Add extra tokens
    tokens.sort(key=lambda tup: tup[1])
    return tokens, keywords, punctuators


def info():
    return "//\n// This file is generated by the 'tokens.py' script.\n//\n\n"


def header_guard_top():
    guard_name = "_TOKENS_HEADER_"
    guard_string = "\n"
    guard_string += "#ifndef " + guard_name + "\n"
    guard_string += "#define " + guard_name
    return guard_string + "\n\n"


def header_guard_bottom():
    return "#endif\n"


def includes(fnames):
    """Add included files."""
    include_string = ""
    for fname in fnames:
        include_string += "#include " + fname + "\n"
    return include_string + "\n\n"


def namespace(nspaces):
    """Add namespaces."""
    nspace_string = ""
    for nspace in nspaces:
        nspace_string += "namespace " + nspace + "{\n"
    return nspace_string + "\n\n"


def namespace_end(count):
    return "}\n" * count + "\n"


def token_enum(tokens):
    """Add a definition for the TOK enum."""
    enum_string = "enum class " + TOK_ENUM + "\n{\n"
    for tok in tokens:
        SPACE = " " * (12 - len(tok[0]))
        enum_string += TAB + tok[0] + "," + SPACE + "// " + tok[1] + "\n"
    return enum_string + "};\n\n"


def token_struct():
    """Add a definition for the Token struct."""
    struct_string = "struct " + TOK_STRUCT + "{\n"
    struct_string += TAB + TOK_ENUM + " name;\n"
    struct_string += TAB + "std::string value;\n"
    struct_string += TAB + "std::string " + TOSTRING + "() const;\n"
    return struct_string + "};\n\n"


def token_string_proto(separate=False):
    bnr_string = COMMENT_BANNER
    bnr_string += "\n// Functions for printing Tokens and TOKs\n"
    bnr_string += COMMENT_BANNER

    tok_str = "std::string " + TOSTRING_FCN + "(" + TOK_ENUM + " t)"
    tok_tostr = "std::string Token::" + TOSTRING + "() const"
    tok_struct = "std::ostream& operator<<(std::ostream& os, const "
    tok_struct += TOK_STRUCT + "& t)"
    tok_enum = "std::ostream& operator<<(std::ostream& os, const "
    tok_enum += TOK_ENUM + "& t)"

    if not separate:
        proto_string = bnr_string + "\n" + tok_str + ";\n" + tok_struct
        proto_string += ";\n" + tok_enum + ";\n\n"
        return proto_string
    else:
        return bnr_string, tok_str, tok_tostr, tok_struct, tok_enum


def token_string_fcn(tokens):
    """Add a definition for the token toString function."""

    # Get the banner and function definitions
    bnr, tok, token, struct, enum = token_string_proto(True)
    fcn_string = bnr + "\n"

    # Write the TOK_toString function (given TOK return string)
    fcn_string += tok + "\n{\n"

    # Set a constant value for the number of tokens
    fcn_string += TAB + "const auto NUM_TOK = " + str(len(tokens)) + ";\n"

    # An array of TOKs in string form
    array_name = "TOK_STRINGS"
    fcn_string += TAB + "static const std::array<std::string, NUM_TOK> "
    fcn_string += array_name + " = {{"

    line_start = "\n" + TAB + TAB
    col = len(line_start)
    fcn_string += line_start
    for tok in tokens:
        # Make sure that lines do not go over MAX_COL characters
        line = "\"" + tok[0] + "\", "
        if (col + len(line)) > MAX_COL:
            fcn_string += "\n" + TAB + TAB
            col = len(line_start)
        fcn_string += line
        col += len(line)
    fcn_string += "\n" + TAB + "}};\n"

    # Return the string version of the token
    fcn_string += TAB + "return " + array_name + "[static_cast<unsigned>(t)];"
    fcn_string += "\n}\n"

    # The Token toString function
    fcn_string += token + " {\n"
    fcn_string += TAB + "return \"{\" + " + TOSTRING_FCN + "(this->name)"
    fcn_string += " + \":\\\"\" + this->value + \"\\\"}\";\n}\n"

    # The struct output stream function
    fcn_string += struct + " {\n"
    fcn_string += TAB + "os << t." + TOSTRING + "();\n" + TAB
    fcn_string += "return os;\n}\n"

    # The enum output stream function
    fcn_string += enum + " {\n"
    fcn_string += TAB + "os << " + TOSTRING_FCN + "(t);\n" + TAB
    fcn_string += "return os;\n}\n"

    return fcn_string + "\n"


def reserved_keyword_proto(separate=False):
    bnr_string = COMMENT_BANNER
    bnr_string += "\n// Utility function used to check for reserved words\n"
    bnr_string += COMMENT_BANNER

    lst_string = "std::unordered_map<std::string, TOK> "
    lst_string += GETRESERVED_FCN + "()"
    fcn_string = TOK_ENUM + " " + RESERVED_FCN + "(std::string word)"

    if not separate:
        return bnr_string + "\n" + lst_string + ";\n" + fcn_string + ";\n\n"
    else:
        return bnr_string, lst_string, fcn_string


def reserved_keyword_list(keywords, proto):
    """Create a function that returns the keywords."""

    fcn_string = proto + "\n{\n"

    map_name = "TOK_KEYWORDS"
    fcn_string += TAB + "const std::unordered_map<std::string, "
    fcn_string += TOK_ENUM + "> " + map_name + " = {\n"
    for kword in keywords:
        fcn_string += TAB + TAB + "{\"" + kword[1]
        fcn_string += "\", TOK::" + kword[0] + "},\n"
    fcn_string += TAB + "};\n" + TAB + "return " + map_name + ";\n}"

    return fcn_string


def reserved_keyword_fcn(keywords):
    """Add a definition for the reserved keyword function."""

    # Get the banner and function definitions
    bnr, lst, fcn = reserved_keyword_proto(True)
    fcn_string = bnr + "\n"

    # Write the get list function
    fcn_string += reserved_keyword_list(keywords, lst) + "\n"

    # Function definition
    fcn_string += fcn + "\n{\n"

    # A map from strings to TOKs (from a new function)
    map_name = "TOK_KEYWORDS"
    fcn_string += TAB + "static const auto " + map_name + " = "
    fcn_string += GETRESERVED_FCN + "();\n"

    # Search for the reserved keywrod in the map (UNDEFINED if not found)
    fcn_string += TAB + "auto search = " + map_name + ".find(word);\n"
    fcn_string += TAB + "if (search != " + map_name + ".end()) {\n"
    fcn_string += TAB + TAB + "return search->second;\n"
    fcn_string += TAB + "}\n" + TAB + "else {\n" + TAB + TAB + "return "
    fcn_string += "TOK::UNDEFINED;\n" + TAB + "}\n"
    return fcn_string + "}\n\n"


def punctuator_proto(separate=False):
    bnr_string = COMMENT_BANNER
    bnr_string += "\n// Utility function used to check for punctuators\n"
    bnr_string += COMMENT_BANNER

    lst_string = "std::unordered_map<std::string, TOK> "
    lst_string += GETPUNCTS_FCN + "()"
    fcn_string = TOK_ENUM + " " + PUNCT_FCN
    fcn_string += "(std::istream& ts, std::string &punc)"

    if not separate:
        return bnr_string + "\n" + lst_string + ";\n" + fcn_string + ";\n\n"
    else:
        return bnr_string, lst_string, fcn_string


def punctuator_list(punctuators, proto):
    """Create a function that returns the punctuators."""

    fcn_string = proto + "\n{\n"

    map_name = "TOK_PUNCTUATORS"
    fcn_string += TAB + "const std::unordered_map<std::string, "
    fcn_string += TOK_ENUM + "> " + map_name + " = {\n"
    for punct in punctuators:
        fcn_string += TAB + TAB + "{\"" + punct[1]
        fcn_string += "\", TOK::" + punct[0] + "},\n"
    fcn_string += TAB + "};\n" + TAB + "return " + map_name + ";\n}"

    return fcn_string


def punctuator_fcn(punctuators):
    """Add a definition for a punctuator function."""

    # Get the banner and function definitions
    bnr, lst, fcn = punctuator_proto(True)
    fcn_string = bnr + "\n"

    # Write the punctuator list function
    fcn_string += punctuator_list(punctuators, lst) + "\n"

    # Function definition
    fcn_string += fcn + "\n{\n"

    # Add a variable for the maximum number of characters in a punctuator
    max_punc_len = max([len(p[1]) for p in punctuators])
    fcn_string += TAB + "static const auto MAX_PUNC_CHARS = "
    fcn_string += str(max_punc_len) + ";\n"
    fcn_string += TAB + "static const auto MAX_ALLOWED_CHARS = 2u;\n"
    fcn_string += TAB + "static_assert(MAX_PUNC_CHARS == MAX_ALLOWED_CHARS);\n"

    # Create the map of strings to punctuator tokens
    map_name = "TOK_PUNCTUATORS"
    fcn_string += TAB + "static const auto " + map_name + " = "
    fcn_string += GETPUNCTS_FCN + "();\n"

    # Search for the function based on how many characters we should search
    fcn_string += """
    auto punc_size = 1u;
    auto return_token = TOK::UNDEFINED;
    auto search = TOK_PUNCTUATORS.find(punc);
    if (search != TOK_PUNCTUATORS.end()) {
        return_token = search->second;
    }

    // Check for longer punctuators (return the longest valid punc)
    for (auto i = punc_size + 1u; i <= MAX_ALLOWED_CHARS; ++i) {
        char c;
        if (ts.get(c)) {
            punc += c;
            search = TOK_PUNCTUATORS.find(punc);
            if (search != TOK_PUNCTUATORS.end()) {
                punc_size = i;
                return_token = search->second;
            }
        }
    }

    // Unget any unused characters
    for (auto i = MAX_ALLOWED_CHARS; i > punc_size; --i) {
        ts.unget();
        punc.pop_back();
    }
    return return_token;"""

    return fcn_string + "\n}\n\n"


def main(ifilename, ofilename):
    """Create the tokens header file."""

    # Read tokens and check for empty file
    token_lines = []
    with open(ifilename, 'r') as tokens_file:
        token_lines = tokens_file.readlines()
    tokens, keywords, punctuators = read_tokens(token_lines)
    if len(tokens) == 0:
        print("Error: no tokens found in the input stream.")
        exit(1)

    # Create and write to the header file
    header_fname = ofilename + ".h"
    with open(header_fname, 'w') as header_file:
        header_file.write(info())
        header_file.write(header_guard_top())
        header_file.write(includes(["<string>", "<unordered_map>"]))
        header_file.write(namespace(["raptor", "lexer"]))
        header_file.write(token_enum(tokens))
        header_file.write(token_struct())
        header_file.write(token_string_proto())
        header_file.write(reserved_keyword_proto())
        header_file.write(punctuator_proto())
        header_file.write(namespace_end(2))
        header_file.write(header_guard_bottom())

    # Create and write to the implementation file
    cpp_fname = ofilename + ".cpp"
    header_base = "\"" + os.path.basename(header_fname) + "\""
    with open(cpp_fname, 'w') as cpp_file:
        cpp_file.write(info())
        cpp_file.write(includes([header_base, "<string>", "<array>",
                                "<unordered_map>", "<iostream>"]))
        cpp_file.write(namespace(["raptor", "lexer"]))
        cpp_file.write(token_string_fcn(tokens))
        cpp_file.write(reserved_keyword_fcn(keywords))
        cpp_file.write(punctuator_fcn(punctuators))
        cpp_file.write(namespace_end(2))

if __name__ == '__main__':

    if (len(sys.argv) != 3):
        print("Error creating 'token' header. Inalid number of CL arguments.")
        exit(1)

    ifilename = sys.argv[1]
    ofilename = sys.argv[2]

    if (ifilename[-3:] != ".in"):
        print("Invalid input filename: \"" + ifilename + "\".")
        exit(1)

    else:
        main(ifilename, ofilename)
